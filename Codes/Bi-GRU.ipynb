{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a5148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vinnu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vinnu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\vinnu\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 703ms/step - accuracy: 0.2120 - loss: 1.7841 - precision: 0.6065 - recall: 0.0044 - val_accuracy: 0.4398 - val_loss: 1.5808 - val_precision: 0.9938 - val_recall: 0.0419 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 747ms/step - accuracy: 0.4496 - loss: 5229538304.0000 - precision: 0.8956 - recall: 0.1362 - val_accuracy: 0.5323 - val_loss: 1.3891 - val_precision: 0.9614 - val_recall: 0.1339 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 947ms/step - accuracy: 0.5458 - loss: 987220416.0000 - precision: 0.8904 - recall: 0.2705 - val_accuracy: 0.5421 - val_loss: 1.3415 - val_precision: 0.9481 - val_recall: 0.1676 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 910ms/step - accuracy: 0.5585 - loss: 1.1930 - precision: 0.8797 - recall: 0.3095 - val_accuracy: 0.5453 - val_loss: 1.3093 - val_precision: 0.9333 - val_recall: 0.1925 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 932ms/step - accuracy: 0.5600 - loss: 289603780608.0000 - precision: 0.8576 - recall: 0.3247 - val_accuracy: 0.5271 - val_loss: 1.3208 - val_precision: 0.9259 - val_recall: 0.1949 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 985ms/step - accuracy: 0.5514 - loss: 1.1936 - precision: 0.8475 - recall: 0.3106 - val_accuracy: 0.5436 - val_loss: 1.2874 - val_precision: 0.9205 - val_recall: 0.2156 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 1s/step - accuracy: 0.5643 - loss: 1.1632 - precision: 0.8509 - recall: 0.3303 - val_accuracy: 0.5466 - val_loss: 1.2683 - val_precision: 0.9161 - val_recall: 0.2288 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 1s/step - accuracy: 0.5667 - loss: 1.1492 - precision: 0.8381 - recall: 0.3442 - val_accuracy: 0.5517 - val_loss: 1.2534 - val_precision: 0.9120 - val_recall: 0.2390 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 1s/step - accuracy: 0.5702 - loss: 27744444416.0000 - precision: 0.8318 - recall: 0.3536 - val_accuracy: 0.5441 - val_loss: 1.2571 - val_precision: 0.9094 - val_recall: 0.2367 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 1s/step - accuracy: 0.5622 - loss: 1.1560 - precision: 0.8310 - recall: 0.3464 - val_accuracy: 0.5442 - val_loss: 1.2495 - val_precision: 0.9109 - val_recall: 0.2411 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 1s/step - accuracy: 0.5649 - loss: 3651.1438 - precision: 0.8325 - recall: 0.3499 - val_accuracy: 0.5486 - val_loss: 1.2411 - val_precision: 0.9075 - val_recall: 0.2481 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m 74/239\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:24\u001b[0m 1s/step - accuracy: 0.5868 - loss: 1.1233 - precision: 0.8238 - recall: 0.3681"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK for text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# TensorFlow and Keras for building and training neural network models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization,\n",
    "                                     Activation, Flatten, LSTM, Bidirectional, GRU)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Sklearn for preprocessing and model evaluation\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = 'cyberbullying_tweets.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Clean and preprocess text\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_and_preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_text = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_text.extend(tokens)\n",
    "    return \" \".join(processed_text)\n",
    "\n",
    "df['cleaned_tweet_text'] = df['tweet_text'].apply(clean_and_preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelBinarizer()\n",
    "y_encoded = encoder.fit_transform(df['cyberbullying_type'])\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['cleaned_tweet_text'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "X = tokenizer.texts_to_sequences(df['cleaned_tweet_text'])\n",
    "maxlen = 200\n",
    "X_padded = pad_sequences(X, maxlen=maxlen)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.20, random_state=42)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embeddings_dictionary = dict()\n",
    "with open('glove.6B.200d.txt', encoding=\"utf8\") as glove_file:\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        if vector_dimensions.shape[0] == 200:\n",
    "            embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 200))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        if index < vocab_size:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# Define the model with Bi-GRU\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=maxlen, trainable=False),\n",
    "    Bidirectional(GRU(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_encoded.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=30, batch_size=128,\n",
    "                    callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "\n",
    "# Save the model in the .h5 format\n",
    "model_save_path_h5 = 'Cyber_Bullying_model_bigrus_withcallbacks.h5'\n",
    "model.save(model_save_path_h5)\n",
    "print(f\"Model saved to {model_save_path_h5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc236f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
